<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Digital Dance Group</title>
    <link>http://research.mlabdance.com/</link>
    <description>Recent content on Digital Dance Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jul 2015 17:48:26 +0900</lastBuildDate>
    <atom:link href="http://research.mlabdance.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>第4回早稲田大学アプリケーションコンテスト</title>
      <link>http://research.mlabdance.com/news/2015-07-31-app/</link>
      <pubDate>Fri, 31 Jul 2015 17:48:26 +0900</pubDate>
      
      <guid>http://research.mlabdance.com/news/2015-07-31-app/</guid>
      <description>

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;img src=&#34;../../img/news/1.jpg&#34; alt=&#34;&#34; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;第4回早稲田大学アプリケーションコンテスト:363f5cd9fd656ecdb5b042245c9fa8b0&#34;&gt;第4回早稲田大学アプリケーションコンテスト&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;学内のアプリケーションコンテストにて，&amp;rdquo;森島研ダンス班&amp;rdquo;が優勝しました．&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.waseda.jp/rps/incubation/event/appcon4/index.html&#34;&gt;第4回早稲田大学アプリケーションコンテスト最終発表会&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Expressive Dance Motion Generation</title>
      <link>http://research.mlabdance.com/projects/Expressive_Dance_Motion_Generation/</link>
      <pubDate>Sun, 05 Jul 2015 17:48:26 +0900</pubDate>
      
      <guid>http://research.mlabdance.com/projects/Expressive_Dance_Motion_Generation/</guid>
      <description>

&lt;h2 id=&#34;segmentation-of-continuous-dance-motion:28898c2c586d726902e9974161dc9bab&#34;&gt;Segmentation of continuous Dance Motion&lt;/h2&gt;

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;img src=&#34;../../img/projects/0.jpg&#34; alt=&#34;&#34; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;abstract:28898c2c586d726902e9974161dc9bab&#34;&gt;Abstract&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;The power of expression such as accent in motion and movement of arms is an indispensable factor in dance performance because there is a large difference in appearance between natural dance and expressive motions. Needless to say, expressive dance motion makes a great impression on viewers. However, creating such a dance motion is challenging because most of the creators have little knowledge about dance performance. Therefore, there is a demand for a system that generates expressive dance motion with ease. Tsuruta et al. [2010] generated expressive dance motion by changing only the speed of input motion or altering joint angles. However, the power of expression was not evaluated with certainty, and the generated motion did not synchronize with music. Therefore, the generated motion did not always satisfy the viewers.
Therefore, we propose a method that transforms arbitrary dance motion into more expressive motion by filtering in accent and power. Original dance motion is divided into segments and converted to expressive dance to keep the original tempo. The expression conversion rule is extracted by analyzing motion capture data from training dance motions that include neutral and expressive motions labeled by subjective assessment.&lt;/p&gt;

&lt;h3 id=&#34;demo:28898c2c586d726902e9974161dc9bab&#34;&gt;Demo&lt;/h3&gt;

&lt;hr /&gt;

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;iframe width=&#34;650&#34; height=&#34;350&#34; src=&#34;https://www.youtube.com/embed/1zRfETCZaho&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;publication:28898c2c586d726902e9974161dc9bab&#34;&gt;Publication&lt;/h3&gt;

&lt;hr /&gt;

&lt;div class=&#34;publication&#34;&gt;
&lt;p&gt;
&lt;b&gt;&lt;i&gt;Expressive Dance Motion Generation&lt;/i&gt;&lt;/b&gt;&lt;br&gt;
Narumi Okada, Kazuki Okami, Tsukasa Fukusato, Naoya Iwamoto and Shigeo Morishima.&lt;br&gt;
ACM SIGGRAPH 2013, Posters, 4, Anaheim, 2013.07.21-25.
&lt;a href=&#34;&#34;&gt;&lt;i class=&#34;fa fa-file-pdf-o text-primary&#34;&gt;&lt;/i&gt;.pdf&lt;/a&gt;&lt;br&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Physically-based Character Animation</title>
      <link>http://research.mlabdance.com/projects/Physically_based_Character_Animation/</link>
      <pubDate>Sun, 05 Jul 2015 17:48:26 +0900</pubDate>
      
      <guid>http://research.mlabdance.com/projects/Physically_based_Character_Animation/</guid>
      <description>

&lt;h2 id=&#34;segmentation-of-continuous-dance-motion:f35d1b58ab5b49c8c21a4d60340929e6&#34;&gt;Segmentation of continuous Dance Motion&lt;/h2&gt;

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;img src=&#34;../../img/projects/iwamoto.jpg&#34; alt=&#34;&#34; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;abstract:f35d1b58ab5b49c8c21a4d60340929e6&#34;&gt;Abstract&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;Multi-layer Lattice Model for Real-Time Dynamic Character Deformation&lt;/p&gt;

&lt;h3 id=&#34;demo:f35d1b58ab5b49c8c21a4d60340929e6&#34;&gt;Demo&lt;/h3&gt;

&lt;hr /&gt;

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;iframe width=&#34;650&#34; height=&#34;350&#34; src=&#34;https://www.youtube.com/embed/xzWOBEhk0RA&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;publication:f35d1b58ab5b49c8c21a4d60340929e6&#34;&gt;Publication&lt;/h3&gt;

&lt;hr /&gt;

&lt;div class=&#34;publication&#34;&gt;
&lt;p&gt;
&lt;b&gt;&lt;i&gt;Dance Motion Segmentation Method Based on Choreographic Primitives&lt;/i&gt;&lt;/b&gt;&lt;br&gt;
Narumi Okada, Naoya Iwamoto, Tsukasa Fukusato and Shigeo Morishima.&lt;br&gt;
In Proceedings of the 10th International Conference on Computer Graphics Theory and Applications (GRAPP 2015), 47, Berlin, 2015.03.11-14. 
&lt;a href=&#34;&#34;&gt;&lt;i class=&#34;fa fa-file-pdf-o text-primary&#34;&gt;&lt;/i&gt;.pdf&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;ttp://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005304303320339&#34;&gt;DigitalLibrary&lt;/a&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visual Computing / グラフィクスとCAD 合同シンポジウム 2015</title>
      <link>http://research.mlabdance.com/news/2015-06-28-gcad/</link>
      <pubDate>Sun, 28 Jun 2015 17:48:26 +0900</pubDate>
      
      <guid>http://research.mlabdance.com/news/2015-06-28-gcad/</guid>
      <description>

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;img src=&#34;../../img/news/2.jpg&#34; alt=&#34;&#34; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;visual-computing-グラフィクスとcad-合同シンポジウム-2015:89281347f97e3243b7adda6a0cace020&#34;&gt;Visual Computing / グラフィクスとCAD 合同シンポジウム 2015&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;姫路で開催されたVCGCADシンポジウム2015にて，楽曲の印象推定とダンスモーションを考慮した表情自動生成に関する発表を行ないました．&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;publication&#34;&gt;
&lt;p&gt;
&lt;b&gt;&lt;i&gt;ダンスにシンクロした楽曲印象推定によるダンスキャラクタの表情アニメーション生成手法の提案&lt;/i&gt;&lt;/b&gt;&lt;br&gt;
朝比奈わかな, 岡田成美, 岩本尚也, 増田太郎, 福里司, 森島繁生&lt;br&gt;
Visual Computing/GCAD合同シンポジウム2015, 姫路, 2015.06.28-30.
&lt;a href=&#34;&#34;&gt;&lt;i class=&#34;fa fa-file-pdf-o text-primary&#34;&gt;&lt;/i&gt;.pdf&lt;/a&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ipsj-gcad.sakura.ne.jp/vc2015/program.html&#34;&gt;Visual Computing / グラフィクスとCAD 合同シンポジウム 2015&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Synthesis Facial Animation</title>
      <link>http://research.mlabdance.com/projects/Synthesis_Facial_Animation/</link>
      <pubDate>Thu, 18 Jun 2015 17:48:26 +0900</pubDate>
      
      <guid>http://research.mlabdance.com/projects/Synthesis_Facial_Animation/</guid>
      <description>

&lt;h2 id=&#34;synthesis-facial-animation:045fc8e824be06a31b090edd6e831e59&#34;&gt;Synthesis Facial Animation&lt;/h2&gt;

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;img src=&#34;../../img/projects/asahina.jpg&#34; alt=&#34;&#34; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;abstract:045fc8e824be06a31b090edd6e831e59&#34;&gt;Abstract&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;本研究は，キャラクタの表情を自動合成するための，ダンスモーションに同期した楽曲印象推定方法を    提案する．手順として，楽曲に関しては，音色やリズムに関する特徴量を，ダンスに関しては，ラバンの身体理論に基づいたモーション特徴    量を用いる．また，VA平面と呼ばれる感情平面を基に印象の軌跡を推定し，キャラクタの表情を合成した．本手法を用いて，キャラクタに精    細な表情を自動付与することで，簡易的に作品の印象を強調したダンス動画の生成を可能にした．&lt;/p&gt;

&lt;h3 id=&#34;demo:045fc8e824be06a31b090edd6e831e59&#34;&gt;Demo&lt;/h3&gt;

&lt;hr /&gt;

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;iframe width=&#34;650&#34; height=&#34;350&#34; src=&#34;https://www.youtube.com/embed/rREzw34CjXw&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;publication:045fc8e824be06a31b090edd6e831e59&#34;&gt;Publication&lt;/h3&gt;

&lt;hr /&gt;

&lt;div class=&#34;publication&#34;&gt;
&lt;p&gt;
&lt;b&gt;&lt;i&gt;ダンスにシンクロした楽曲印象推定によるダンスキャラクタの表情アニメーション生成手法の提案&lt;/i&gt;&lt;/b&gt;&lt;br&gt;
朝比奈わかな, 岡田成美, 岩本尚也, 増田太郎, 福里司, 森島繁生&lt;br&gt;
Visual Computing/GCAD合同シンポジウム2015, 姫路, 2015.06.28-30.
&lt;a href=&#34;&#34;&gt;&lt;i class=&#34;fa fa-file-pdf-o text-primary&#34;&gt;&lt;/i&gt;.pdf&lt;/a&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
&lt;b&gt;&lt;i&gt;ダンスモーションに同期した表情自動合成のための楽曲印象解析手法の提案&lt;/i&gt;&lt;/b&gt;&lt;br&gt;
朝比奈わかな, 岡田成美, 岩本尚也, 増田太郎, 福里司, 森島繁生&lt;br&gt;
情報処理学会 第77回全国大会, 2S-08, 京都, 2015.03.17-19.&lt;b&gt;学生奨励賞&lt;/b&gt;
&lt;a href=&#34;&#34;&gt;&lt;i class=&#34;fa fa-file-pdf-o text-primary&#34;&gt;&lt;/i&gt;.pdf&lt;/a&gt;&lt;br&gt;
&lt;br&gt;

&lt;p&gt;
&lt;b&gt;&lt;i&gt;ダンスモーションにシンクロした音楽印象推定手法の提案とダンサーの表情自動合成への応用&lt;/i&gt;&lt;/b&gt;&lt;br&gt;
比奈わかな, 岡田成美, 岩本尚也, 増田太郎, 福里司, 森島繁生&lt;br&gt;
情報処理学会 第106回音楽情報処理科学研究会, 23, 山梨, 2015.03.02-03.&lt;/b&gt;
&lt;a href=&#34;&#34;&gt;&lt;i class=&#34;fa fa-file-pdf-o text-primary&#34;&gt;&lt;/i&gt;.pdf&lt;/a&gt;&lt;br&gt;
&lt;br&gt;

&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Segmentation of continuous Dance Motion</title>
      <link>http://research.mlabdance.com/projects/Segmentation_of_continuous_Dance_Motion/</link>
      <pubDate>Sat, 14 Mar 2015 17:48:26 +0900</pubDate>
      
      <guid>http://research.mlabdance.com/projects/Segmentation_of_continuous_Dance_Motion/</guid>
      <description>

&lt;h2 id=&#34;segmentation-of-continuous-dance-motion:6ecb60fe69374065ef98b3ac6d54f120&#34;&gt;Segmentation of continuous Dance Motion&lt;/h2&gt;

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;img src=&#34;../../img/projects/okada.jpg&#34; alt=&#34;サムネイル&#34; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;abstract:6ecb60fe69374065ef98b3ac6d54f120&#34;&gt;Abstract&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;Data-driven animation using a large human motion database enables the programing of various n    atural human motions. While the development of a motion capture system allows the acquisition of realistic human motion, segmen    ting the captured motion into a series of primitive motions for the construction of a motion database is necessary. Although mo    st segmentation methods have focused on periodic motion, e.g., walking and jogging, segmenting non-periodic and asymmetrical mo    tions such as dance performance, remains a challenging problem. In this paper, we present a specialized segmentation approach f    or human dance motion. Our approach consists of three steps based on the assumption that human dance motion is composed of cons    ecutive choreographic primitives. First, we perform an investigation based on dancer perception to determine segmentation compo    nents. After professional dancers have selected segmentation sequences, we use their selected sequences to define rules for the     segmentation of choreographic primitives. Finally, the accuracy of our approach is verified by a user-study, and we thereby sh    ow that our approach is superior to existing segmentation methods. Through three steps, we demonstrate automatic dance motion s    ynthesis based on the choreographic primitives obtained.&lt;/p&gt;

&lt;h3 id=&#34;demo:6ecb60fe69374065ef98b3ac6d54f120&#34;&gt;Demo&lt;/h3&gt;

&lt;hr /&gt;

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;iframe width=&#34;650&#34; height=&#34;350&#34; src=&#34;https://www.youtube.com/embed/xzWOBEhk0RA&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;publication:6ecb60fe69374065ef98b3ac6d54f120&#34;&gt;Publication&lt;/h3&gt;

&lt;hr /&gt;

&lt;div class=&#34;publication&#34;&gt;
&lt;p&gt;
&lt;b&gt;&lt;i&gt;Dance Motion Segmentation Method Based on Choreographic Primitives&lt;/i&gt;&lt;/b&gt;&lt;br&gt;
Narumi Okada, Naoya Iwamoto, Tsukasa Fukusato and Shigeo Morishima.&lt;br&gt;
In Proceedings of the 10th International Conference on Computer Graphics Theory and Applications (GRAPP 2015), 47, Berlin, 2015.03.11-14. 
&lt;a href=&#34;&#34;&gt;&lt;i class=&#34;fa fa-file-pdf-o text-primary&#34;&gt;&lt;/i&gt;.pdf&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;ttp://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005304303320339&#34;&gt;DigitalLibrary&lt;/a&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>VISIGGRAPP 2015</title>
      <link>http://research.mlabdance.com/news/2015-03-11-visigrapp/</link>
      <pubDate>Wed, 11 Mar 2015 17:48:26 +0900</pubDate>
      
      <guid>http://research.mlabdance.com/news/2015-03-11-visigrapp/</guid>
      <description>

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;img src=&#34;../../img/news/3.jpg&#34; alt=&#34;&#34; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;visiggrapp-2015:1547087e5d89505a5761874437333caf&#34;&gt;VISIGGRAPP 2015&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;ベルリンで開催されたVISIGGRAPP 2015にて，ダンスモーションのセグメント分割に関する研究発表を行ないました．&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;publication&#34;&gt;
&lt;p&gt;
&lt;b&gt;&lt;i&gt;Dance Motion Segmentation Method Based on Choreographic Primitives&lt;/i&gt;&lt;/b&gt;&lt;br&gt;
Narumi Okada, Naoya Iwamoto, Tsukasa Fukusato and Shigeo Morishima.&lt;br&gt;
In Proceedings of the 10th International Conference on Computer Graphics Theory and Applications (GRAPP 2015), 47, Berlin, 2015.03.11-14.
&lt;a href=&#34;&#34;&gt;&lt;i class=&#34;fa fa-file-pdf-o text-primary&#34;&gt;&lt;/i&gt;.pdf&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;ttp://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005304303320339&#34;&gt;DigitalLibrary&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.visigrapp.org/&#34;&gt;VISIGGRAPP&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NHK Eテレ「テクネ 映像の教室」</title>
      <link>http://research.mlabdance.com/news/2014-12-28-techne/</link>
      <pubDate>Sun, 28 Dec 2014 17:48:26 +0900</pubDate>
      
      <guid>http://research.mlabdance.com/news/2014-12-28-techne/</guid>
      <description>

&lt;div class=&#34;embedded-image-wrapper&#34;&gt;
    &lt;div class=&#34;embedded-image-container&#34;&gt;
        &lt;img src=&#34;../../img/news/0.jpg&#34; alt=&#34;&#34; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;nhk-eテレ-テクネ-映像の教室:f22e87e074efba8736ed264d06804e46&#34;&gt;NHK Eテレ「テクネ 映像の教室」&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;NHK Eテレ「テクネ 映像の教室」にて，岩本がダンサーのジェスチャー認識と弾性体シミュレーションプログラムを開発しました．&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.nhk.or.jp/bijutsu/techne/try/content.html#programming&#34;&gt;テクネ 映像の教室&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>